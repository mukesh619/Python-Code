{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie-review-sentiment-analysis\\sampleSubmission.csv\n",
      "movie-review-sentiment-analysis\\test.tsv\n",
      "movie-review-sentiment-analysis\\train.tsv\n"
     ]
    }
   ],
   "source": [
    "for dirname,_,filenames in os.walk('movie-review-sentiment-analysis'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.read_csv(r\"movie-review-sentiment-analysis\\train.tsv\",sep='\\t')\n",
    "df_test=pd.read_csv(r\"movie-review-sentiment-analysis\\test.tsv\",sep='\\t')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport csv\\nwith open(r\"movie-review-sentiment-analysis\\train.tsv\") as tsvfile:\\n    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\\n    for line in tsvreader:\\n        print(line)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import csv\n",
    "with open(r\"movie-review-sentiment-analysis\\train.tsv\") as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        print(line)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhraseId      0\n",
       "SentenceId    0\n",
       "Phrase        0\n",
       "Sentiment     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060, 4)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem=WordNetLemmatizer()\n",
    "stop=set(stopwords.words(\"english\"))\n",
    "df_train.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in (df_train['Phrase'][:100]):\\n    #review=i.split()\\n    #review=i.lower()\\n    #review=review.split()\\n    review=re.sub('[^a-zA-Z]',' ',i)\\n    review=review.split()\\n    #review=review.lower()\\n    #review=re.sub('[^a-zA-Z]',' ',text)\\n    review=[lem.lemmatize(j) for j in review if not j in stopwords.words('english')]\\n    review=' '.join(review)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for i in (df_train['Phrase'][:100]):\n",
    "    #review=i.split()\n",
    "    #review=i.lower()\n",
    "    #review=review.split()\n",
    "    review=re.sub('[^a-zA-Z]',' ',i)\n",
    "    review=review.split()\n",
    "    #review=review.lower()\n",
    "    #review=re.sub('[^a-zA-Z]',' ',text)\n",
    "    review=[lem.lemmatize(j) for j in review if not j in stopwords.words('english')]\n",
    "    review=' '.join(review)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the shit and then stemming it\n",
    "def prep(text):\n",
    "    review=re.sub('[^a-zA-Z]',' ',text)\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[ lem.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "    review=' '.join(review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cleaned_text']=df_train['Phrase'].apply(lambda x: prep(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['cleaned_text']=df_test['Phrase'].apply(lambda x: prep(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    series escapade demonstrating adage good goose...\n",
       "1       series escapade demonstrating adage good goose\n",
       "2                                               series\n",
       "3                                                     \n",
       "4                                               series\n",
       "5              escapade demonstrating adage good goose\n",
       "6                                                     \n",
       "7              escapade demonstrating adage good goose\n",
       "8                                             escapade\n",
       "9                       demonstrating adage good goose\n",
       "Name: cleaned_text, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['cleaned_text'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.509945\n",
       "3    0.210989\n",
       "1    0.174760\n",
       "4    0.058990\n",
       "0    0.045316\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.Sentiment.value_counts()/len(df_train.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_train.cleaned_text\n",
    "y=df_train.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_test=vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_train=vec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see if all the words in the test set occurs in the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Vocabulary Size: 15240\n",
      "Test Set Vocabulary Size: 9546\n",
      "Number of Words that occur in both: 7057\n"
     ]
    }
   ],
   "source": [
    "cv1 = TfidfVectorizer()\n",
    "cv1.fit(df_train[\"Phrase\"])\n",
    "\n",
    "cv2 = TfidfVectorizer()\n",
    "cv2.fit(df_test[\"Phrase\"])\n",
    "\n",
    "print(\"Train Set Vocabulary Size:\", len(cv1.vocabulary_))\n",
    "print(\"Test Set Vocabulary Size:\", len(cv2.vocabulary_))\n",
    "print(\"Number of Words that occur in both:\", len(set(cv1.vocabulary_.keys()).intersection(set(cv2.vocabulary_.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((124848, 13598), (31212, 12409))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_train.shape,vec_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "log=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 12409 features per sample; expecting 13598",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-cba9998e193f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpred_log\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \"\"\"\n\u001b[1;32m--> 289\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 270\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 12409 features per sample; expecting 13598"
     ]
    }
   ],
   "source": [
    "log.fit(vec_train,y_train)\n",
    "pred_log=log.predict(vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.6185121107266436\n",
      "confusion_matrix\n",
      " [[  189   695   518    41     2]\n",
      " [  114  1674  3372   238     3]\n",
      " [   30   688 14265   916    31]\n",
      " [    4   148  3389  2778   221]\n",
      " [    0    23   413  1061   399]]\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy_score\",accuracy_score(y_test,pred_log))\n",
    "print(\"confusion_matrix\\n\",confusion_matrix(y_test,pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Embedding,LSTM,Conv1D,GlobalMaxPooling1D,Flatten,MaxPooling1D,GRU,SpatialDropout1D,Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import sequence,text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding number of unique words in train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13619"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words=' '.join(X_train)\n",
    "all_words1=word_tokenize(all_words)\n",
    "dist=FreqDist(all_words1)\n",
    "num_unique_word=len(dist)\n",
    "num_unique_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding max length of a review in train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_len=[]\n",
    "for text in X_train:\n",
    "    word=word_tokenize(text)\n",
    "    l=len(word)\n",
    "    r_len.append(l)\n",
    "    \n",
    "MAX_REVIEW_LEN=np.max(r_len)\n",
    "MAX_REVIEW_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = num_unique_word\n",
    "max_words = MAX_REVIEW_LEN\n",
    "batch_size = 128\n",
    "epochs = 3\n",
    "num_classes=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(list(X_train))\n",
    "X_train1 = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_test)\n",
    "X_test1 = tokenizer.texts_to_sequences(df_test['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124848, 30) (31212, 30) (66292, 30)\n"
     ]
    }
   ],
   "source": [
    "X_train1 = sequence.pad_sequences(X_train1, maxlen=max_words)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=max_words)\n",
    "X_test1 = sequence.pad_sequences(X_test1, maxlen=max_words)\n",
    "print(X_train1.shape,X_val.shape,X_test1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         1361900   \n",
      "_________________________________________________________________\n",
      "unified_lstm_2 (UnifiedLSTM) (None, None, 64)          42240     \n",
      "_________________________________________________________________\n",
      "unified_lstm_3 (UnifiedLSTM) (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 1,416,721\n",
      "Trainable params: 1,416,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1=Sequential()\n",
    "model1.add(Embedding(max_features,100,mask_zero=True))\n",
    "model1.add(LSTM(64,dropout=0.4, recurrent_dropout=0.4,return_sequences=True))\n",
    "model1.add(LSTM(32,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
    "model1.add(Dense(num_classes,activation='softmax'))\n",
    "model1.compile(loss='sparse_categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/3\n",
      "124848/124848 [==============================] - 149s 1ms/sample - loss: 1.0700 - accuracy: 0.5861 - val_loss: 0.8974 - val_accuracy: 0.6388\n",
      "Epoch 2/3\n",
      "124848/124848 [==============================] - 154s 1ms/sample - loss: 0.8467 - accuracy: 0.6573 - val_loss: 0.8608 - val_accuracy: 0.6534\n",
      "Epoch 3/3\n",
      "124848/124848 [==============================] - 151s 1ms/sample - loss: 0.7916 - accuracy: 0.6795 - val_loss: 0.8557 - val_accuracy: 0.6553\n"
     ]
    }
   ],
   "source": [
    "history1=model1.fit(X_train1, y_train, validation_data=(X_val, y_test),epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66292/66292 [==============================] - 29s 445us/sample\n"
     ]
    }
   ],
   "source": [
    "y_pred1=model1.predict_classes(X_test1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2, ..., 2, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31212/31212 [==============================] - 5s 174us/sample - loss: 0.8557 - accuracy: 0.6553\n",
      "Test score: 0.8556830746599242\n",
      "Test accuracy: 0.6553249\n"
     ]
    }
   ],
   "source": [
    "score, acc = model1.evaluate(X_val, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 30, 100)           1361900   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 30, 64)            19264     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,390,129\n",
      "Trainable params: 1,390,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2= Sequential()\n",
    "model2.add(Embedding(max_features,100,input_length=max_words))\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Conv1D(64,kernel_size=3,padding='same',activation='relu',strides=1))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "\n",
    "model2.add(Dense(128,activation='relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "\n",
    "model2.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/3\n",
      "124848/124848 [==============================] - 45s 362us/sample - loss: 1.0139 - accuracy: 0.5964 - val_loss: 0.8808 - val_accuracy: 0.6414\n",
      "Epoch 2/3\n",
      "124848/124848 [==============================] - 47s 380us/sample - loss: 0.7985 - accuracy: 0.6729 - val_loss: 0.8431 - val_accuracy: 0.6544\n",
      "Epoch 3/3\n",
      "124848/124848 [==============================] - 52s 419us/sample - loss: 0.7231 - accuracy: 0.7025 - val_loss: 0.8275 - val_accuracy: 0.6596\n"
     ]
    }
   ],
   "source": [
    "history2=model2.fit(X_train1, y_train, validation_data=(X_val, y_test),epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66292/66292 [==============================] - 4s 66us/sample\n"
     ]
    }
   ],
   "source": [
    "y_pred2=model2.predict_classes(X_test1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN +GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 30, 100)           1361900   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 30, 64)            19264     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "unified_gru (UnifiedGRU)     (None, 15, 128)           74496     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               245888    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,702,193\n",
      "Trainable params: 1,702,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3= Sequential()\n",
    "model3.add(Embedding(max_features,100,input_length=max_words))\n",
    "model3.add(Conv1D(64,kernel_size=3,padding='same',activation='relu'))\n",
    "model3.add(MaxPooling1D(pool_size=2))\n",
    "model3.add(Dropout(0.25))\n",
    "model3.add(GRU(128,return_sequences=True))\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(128,activation='relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(5,activation='softmax'))\n",
    "model3.compile(loss='sparse_categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/3\n",
      "124848/124848 [==============================] - 88s 701us/sample - loss: 1.0431 - accuracy: 0.5846 - val_loss: 0.8826 - val_accuracy: 0.6348\n",
      "Epoch 2/3\n",
      "124848/124848 [==============================] - 86s 690us/sample - loss: 0.8274 - accuracy: 0.6625 - val_loss: 0.8356 - val_accuracy: 0.6520\n",
      "Epoch 3/3\n",
      "124848/124848 [==============================] - 80s 642us/sample - loss: 0.7496 - accuracy: 0.6878 - val_loss: 0.8286 - val_accuracy: 0.6585\n"
     ]
    }
   ],
   "source": [
    "history3=model3.fit(X_train1, y_train, validation_data=(X_val, y_test),epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66292/66292 [==============================] - 14s 204us/sample\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred3=model3.predict_classes(X_test1, verbose=1)\n",
    "y_pred3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 30, 100)           1361900   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               176640    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 1,539,825\n",
      "Trainable params: 1,539,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "\n",
    "model4.add(Embedding(max_features, 100, input_length=max_words))\n",
    "model4.add(SpatialDropout1D(0.25))\n",
    "model4.add(Bidirectional(GRU(128)))\n",
    "model4.add(Dropout(0.5))\n",
    "\n",
    "model4.add(Dense(5, activation='softmax'))\n",
    "model4.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/3\n",
      "124848/124848 [==============================] - 150s 1ms/sample - loss: 1.0087 - accuracy: 0.5948 - val_loss: 0.8827 - val_accuracy: 0.6376\n",
      "Epoch 2/3\n",
      "124848/124848 [==============================] - 153s 1ms/sample - loss: 0.8245 - accuracy: 0.6618 - val_loss: 0.8445 - val_accuracy: 0.6568\n",
      "Epoch 3/3\n",
      "124848/124848 [==============================] - 153s 1ms/sample - loss: 0.7583 - accuracy: 0.6867 - val_loss: 0.8364 - val_accuracy: 0.6620\n"
     ]
    }
   ],
   "source": [
    "history4=model4.fit(X_train1, y_train, validation_data=(X_val, y_test),epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66292/66292 [==============================] - 28s 421us/sample\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred4=model4.predict_classes(X_test1, verbose=1)\n",
    "y_pred4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "def get_embed_mat(EMBEDDING_FILE, max_features,embed_dim):\n",
    "    # word vectors\n",
    "    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding='utf8'))\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    # embedding matrix\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = min(max_features, len(word_index) + 1)\n",
    "    all_embs = np.stack(embeddings_index.values()) #for random init\n",
    "    embedding_matrix = np.random.normal(all_embs.mean(), all_embs.std(), \n",
    "                                        (num_words, embed_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    max_features = embedding_matrix.shape[0]\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/glove6b100dtxt/glove.6B.100d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-44b0c6faa2c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mEMBEDDING_FILE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../input/glove6b100dtxt/glove.6B.100d.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;31m#word vector dim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0membedding_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_embed_mat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEMBEDDING_FILE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-69-549f0176ce05>\u001b[0m in \u001b[0;36mget_embed_mat\u001b[1;34m(EMBEDDING_FILE, max_features, embed_dim)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_embed_mat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEMBEDDING_FILE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# word vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0membeddings_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_coefs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEMBEDDING_FILE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Found %s word vectors.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/glove6b100dtxt/glove.6B.100d.txt'"
     ]
    }
   ],
   "source": [
    "# embedding matrix\n",
    "EMBEDDING_FILE = '../input/glove6b100dtxt/glove.6B.100d.txt'\n",
    "embed_dim = 100 #word vector dim\n",
    "embedding_matrix = get_embed_mat(EMBEDDING_FILE,max_features,embed_dim)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Embedding(max_features, embed_dim, input_length=X_train.shape[1],weights=[embedding_matrix],trainable=True))\n",
    "model5.add(SpatialDropout1D(0.25))\n",
    "model5.add(Bidirectional(GRU(128,return_sequences=True)))\n",
    "model5.add(Bidirectional(GRU(64,return_sequences=False)))\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(num_classes, activation='softmax'))\n",
    "model5.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history5=model5.fit(X_train1, y_train, validation_data=(X_val, y_test),epochs=4, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred5=model5.predict_classes(X_test1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model1  model2  model3  model4\n",
       "0       3       3       3       3\n",
       "1       3       3       3       3\n",
       "2       2       2       2       2\n",
       "3       3       3       3       3\n",
       "4       3       3       2       3"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_all=pd.DataFrame({'model1':y_pred1,'model2':y_pred2,'model3':y_pred3,'model4':y_pred4})\n",
    "pred_mode=sub_all.agg('mode',axis=1)[0].values\n",
    "sub_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mode=[int(i) for i in pred_mode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
